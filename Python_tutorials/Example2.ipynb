{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load python modules : Plot, image , numerical python \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Set a useful font library\n",
    "font1 = {'size'   : 20, 'family':'STIXGeneral'}\n",
    "#Colors for plotting \n",
    "colors = ['teal', 'yellowgreen', 'gold']\n",
    "#line width for plottting\n",
    "lw = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some examples to investigate \n",
    "\n",
    "exampleno = 1\n",
    "\n",
    "#Columns: k , P(k) w BAO feature , P(k) w/o BAO feature\n",
    "fr_exp = np.loadtxt('data/class/fr_mlg/'+str(exampleno)+'.txt')\n",
    "gr_exp = np.loadtxt('data/class/gr_mlg/'+str(exampleno)+'.txt')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "#Pure spectra \n",
    "plt.plot(fr_exp[:,0],fr_exp[:,1], color='r')\n",
    "plt.plot(gr_exp[:,0],gr_exp[:,1], color='b')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.ylabel(' $P(k)$ ',**font1)\n",
    "plt.xlabel('$k [h/{\\\\rm Mpc}]$', **font1)\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "#BAO signal \n",
    "plt.plot(fr_exp[:,0],fr_exp[:,1]/fr_exp[:,2], color='r', label='f(R)')\n",
    "plt.plot(gr_exp[:,0],gr_exp[:,1]/gr_exp[:,2], color='b', label='GR')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.ylabel(' BAO signal ',**font1)\n",
    "plt.xlabel('$k [h/{\\\\rm Mpc}]$', **font1)\n",
    "plt.legend(loc='best',prop={'size': 16})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1:  Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum and minimum index to find maximum and minimum of spectrum - our selected feature\n",
    "maxi = 20\n",
    "mini = 10\n",
    "\n",
    "kvalues = fr_exp[mini:maxi,0]\n",
    "print(kvalues)\n",
    "\n",
    "#Plot the range over which we are defining our features \n",
    "plt.plot(fr_exp[:,0],fr_exp[:,1]/fr_exp[:,2], color='r', label='f(R)')\n",
    "plt.plot(gr_exp[:,0],gr_exp[:,1]/gr_exp[:,2], color='b', label='GR')\n",
    "\n",
    "plt.axvline(x=kvalues[0], ymin=0., ymax=10,color='k',linestyle='dotted')\n",
    "plt.axvline(x=kvalues[-1], ymin=0., ymax=10,color='k',linestyle='dotted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct feature vector, test set and cross-validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of examples per class \n",
    "import os\n",
    "path, dirs, files = next(os.walk(\"./data/class/fr_mlg\"))\n",
    "file_count = len(files)\n",
    "\n",
    "# number of files to be included in test set \n",
    "test_set = 101\n",
    "# number of files to be included in cross-validation set \n",
    "cv_set = 201\n",
    "# Examples to omit\n",
    "omitted_set = 51\n",
    "\n",
    "# Final training set size \n",
    "m = file_count-test_set-cv_set-omitted_set\n",
    "\n",
    "\n",
    "# Define an array to store the various values of P(k) at a low k and another at a high k\n",
    "high_fr = []\n",
    "high_gr = []\n",
    "low_fr = []\n",
    "low_gr = []\n",
    "\n",
    "# Load the P(k) values per k \n",
    "for i in range(m):\n",
    "    datfr = np.loadtxt('data/class/fr_mlg/'+str(i+1)+'.txt')\n",
    "    datgr = np.loadtxt('data/class/gr_mlg/'+str(i+1)+'.txt')\n",
    "    \n",
    "    #perform rough feature normalisation \n",
    "    norm = np.mean(datgr[mini:maxi,1])\n",
    "    \n",
    "    # add the features to the feature vector \n",
    "    high_fr.append(np.max(datfr[mini:maxi,1]/norm))\n",
    "    low_fr.append(np.min(datfr[mini:maxi,1]/norm))  \n",
    "    high_gr.append(np.max(datgr[mini:maxi,1]/norm))\n",
    "    low_gr.append(np.min(datgr[mini:maxi,1]/norm))\n",
    "\n",
    "\n",
    "total_low = np.concatenate((low_gr,low_fr), axis=None)\n",
    "total_high = np.concatenate((high_gr,high_fr), axis=None)\n",
    "\n",
    "\n",
    "plt.plot(low_fr,high_fr, linestyle='none',marker='x', label='f(R)')\n",
    "plt.plot(low_gr,high_gr, linestyle='none',marker='o', label='GR')\n",
    "plt.ylabel('Maximum $P(k)$ ',**font1)\n",
    "plt.xlabel('Minimum $P(k)$', **font1)\n",
    "plt.legend(loc='best',prop={'size': 16})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now fit a decision boundary to this. We will probably need a polynomial boundary ... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define logistic function first \n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "xplot = np.linspace(-5,5,100)\n",
    "\n",
    "plt.plot(xplot,sigmoid(xplot))\n",
    "plt.axvline(x=0., ymin=0., ymax=10,color='k')\n",
    "plt.axhline(y=1., xmin=-5, xmax=5,color='k',linestyle='dotted')\n",
    "plt.axhline(y=0., xmin=-5, xmax=5,color='k',linestyle='dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function WITH regularisation term "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our cost function \n",
    "# n is the order of the polynomial \n",
    "# m is the number of training examples\n",
    "\n",
    "# x is an (n+2) x m matrix - (1, x2, x1,  x1^n) per example , x1 and x2 are our features (max and min P(k))\n",
    "# y is an m x 1 matrix - 0 or 1 per example \n",
    "# w is an (n+2) x 1 matrix - (bias, w1, w2, ... w(n+2))\n",
    "# w.T is w transpose \n",
    "\n",
    "def mycost(w,x,y,m,lam):\n",
    "    # define unit matrix\n",
    "    myunity = np.ones(m)\n",
    "    \n",
    "    # Polynomial decision boundary\n",
    "    mypoly = np.matmul(x.T,w)\n",
    "        \n",
    "    # sigmoid of decision boundary curve \n",
    "    sigmoidmat = sigmoid(mypoly)   \n",
    "    \n",
    "    # the two terms of te binomial cost \n",
    "    term1 = np.matmul(y.T,np.log(sigmoidmat))\n",
    "    term2 = np.matmul((myunity-y).T,np.log(myunity-sigmoidmat))\n",
    "    \n",
    "    return -1/m * (term1 + term2) + lam * np.matmul(w,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set order of polynomial, construct the feature matrix and check cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to create feature matrix from x1 and x2 points \n",
    "def map_features(low,high, polyorder):\n",
    "    mylength = len(low)\n",
    "    bias = np.ones(mylength)\n",
    "    training_x = [bias]\n",
    "    for i in range(1,polyorder+1):    \n",
    "        for j in range(0,i):    \n",
    "            ind1 = abs(i-j)\n",
    "            training_x = np.concatenate(( np.array(training_x), [low**ind1 * high**(j)], [high**ind1 * low**(j)] ))\n",
    "    \n",
    "    return training_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vector of 'right' answers \n",
    "training_y = np.concatenate((np.zeros((m)),np.ones((m))))\n",
    "\n",
    "\n",
    "# Set order of polynomial \n",
    "myorder = 8\n",
    "\n",
    "# Test cost function for random initial guess \n",
    "mytrainmat=map_features(total_low,total_high,myorder)\n",
    "\n",
    "# weight matrix \n",
    "winit = np.zeros(len(mytrainmat))\n",
    "\n",
    "# test the cost function\n",
    "mycost(winit, mytrainmat , training_y , (m)*2, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set regularisation parameter and optimise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set regularisation parameter\n",
    "lam = 0.0\n",
    "\n",
    "# Use scipy module to optimise the function using a gradient descent algorithm\n",
    "import scipy.optimize as optimize\n",
    "\n",
    "xopt = optimize.fmin(mycost, winit , args=(mytrainmat, training_y , m*2, lam), xtol=1e-10, disp=True)\n",
    "\n",
    "print(xopt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how we did ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "ax = plt.subplot(1,2,1)\n",
    "\n",
    "#Plot decision boundary\n",
    "\n",
    "# To do that, we need to generate many predictions of X.W  and find the ones that align with 0. \n",
    "# We can make a contour plot to do this\n",
    "\n",
    "# Create a mesh of x and y points from the maximum and minimum of our feature vectors\n",
    "x_min = total_high.min()\n",
    "x_max = total_high.max()\n",
    "y_min = total_low.min()\n",
    "y_max = total_low.max()\n",
    "\n",
    "# Steps between min and max \n",
    "h=0.001\n",
    "\n",
    "# Create the mesh \n",
    "x_grid, y_grid = np.meshgrid(np.arange(y_min, y_max, h), np.arange(x_min, x_max, h))\n",
    "\n",
    "# Map these to a feature vector \n",
    "myfeature = map_features(x_grid.ravel() ,  y_grid.ravel() , myorder) \n",
    "\n",
    "# Get the sigmoid value of these vectors \n",
    "Z = sigmoid(np.matmul(myfeature.T,xopt))\n",
    "# Reshape it for plotting\n",
    "Z = Z.reshape(x_grid.shape)\n",
    "\n",
    "#Plot the contour aligning with a sigmoid value of 0.5 (equivalent to X.W=0) \n",
    "plt.contour(x_grid, y_grid, Z, levels=[0.5])\n",
    "\n",
    "plt.plot(low_fr,high_fr, linestyle='none',marker='x', label='f(R)')\n",
    "plt.plot(low_gr,high_gr, linestyle='none',marker='o', label='GR')\n",
    "\n",
    "plt.ylabel('Maximum $P(k)$ ',**font1)\n",
    "plt.xlabel('Minimum $P(k)$', **font1)\n",
    "plt.legend(loc='best',prop={'size': 16})\n",
    "\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "img = mpimg.imread('data/images/bor3.jpg')\n",
    "plt.imshow(img)\n",
    "\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check cross-validation set accuracy and tune lambda and polynomial order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prediction based on our fit \n",
    "def predictor(x, w):\n",
    "    probability = sigmoid(np.matmul(x.T,w))\n",
    "    return probability\n",
    "    if(probability >= 0.5):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create CV and test data from the unused examples\n",
    "high_frcv = []\n",
    "high_grcv = []\n",
    "low_frcv = []\n",
    "low_grcv = []\n",
    "\n",
    "high_frt = []\n",
    "high_grt = []\n",
    "low_frt = []\n",
    "low_grt = []\n",
    "\n",
    "for i in range(m,m+cv_set-1):\n",
    "    \n",
    "    datfr = np.loadtxt('data/class/fr_mlg/'+str(i+1)+'.txt')\n",
    "    datgr = np.loadtxt('data/class/gr_mlg/'+str(i+1)+'.txt')\n",
    "    \n",
    "    #perform rough feature normalisation \n",
    "    norm = np.mean(datgr[mini:maxi,1])\n",
    "    \n",
    "    high_frcv.append(np.max(datfr[mini:maxi,1]/norm))\n",
    "    low_frcv.append(np.min(datfr[mini:maxi,1]/norm))  \n",
    "    high_grcv.append(np.max(datgr[mini:maxi,1]/norm))\n",
    "    low_grcv.append(np.min(datgr[mini:maxi,1]/norm))\n",
    "\n",
    "    \n",
    "for i in range(m+cv_set, m+cv_set+test_set-1):\n",
    "  \n",
    "    datfr = np.loadtxt('data/class/fr_mlg/'+str(i+1)+'.txt')\n",
    "    datgr = np.loadtxt('data/class/gr_mlg/'+str(i+1)+'.txt')\n",
    "\n",
    "    #perform rough feature normalisation \n",
    "    norm = np.mean(datgr[mini:maxi,1])\n",
    "    \n",
    "    high_frt.append(np.max(datfr[mini:maxi,1]/norm))\n",
    "    low_frt.append(np.min(datfr[mini:maxi,1]/norm))  \n",
    "    high_grt.append(np.max(datgr[mini:maxi,1]/norm))\n",
    "    low_grt.append(np.min(datgr[mini:maxi,1]/norm))\n",
    "\n",
    "    \n",
    "    \n",
    "total_low_cv = np.concatenate((low_grcv,low_frcv), axis=None)\n",
    "total_high_cv = np.concatenate((high_grcv,high_frcv), axis=None)\n",
    "\n",
    "total_low_t = np.concatenate((low_grt,low_frt), axis=None)\n",
    "total_high_t = np.concatenate((high_grt,high_frt), axis=None)\n",
    "\n",
    "\n",
    "# Create X.W vector \n",
    "cv_feature = map_features(total_low_cv , total_high_cv , myorder) \n",
    "t_feature = map_features(total_low_t , total_high_t , myorder) \n",
    "\n",
    "# Create vector of 'right' answers \n",
    "cv_y = np.concatenate((np.zeros((cv_set-1)),np.ones((cv_set-1))))\n",
    "t_y = np.concatenate((np.zeros((test_set-1)),np.ones((test_set-1))))\n",
    "\n",
    "\n",
    "# Calculate the accuracy \n",
    "myacccv = 0\n",
    "myacct = 0\n",
    "\n",
    "for i in range(cv_set*2-2):\n",
    "    mypredcv = round(predictor(cv_feature,xopt)[i])\n",
    "\n",
    "    if(mypredcv==cv_y[i]):\n",
    "            myacccv +=1 \n",
    "   \n",
    "\n",
    "for i in range(test_set*2-2):\n",
    "    mypredt = round(predictor(t_feature,xopt)[i])\n",
    "    if(mypredt==t_y[i]):\n",
    "            myacct +=1 \n",
    "        \n",
    "    \n",
    "print(\"CV Set accuracy:\", myacccv/(cv_set*2-2))\n",
    "print(\"Test Set accuracy:\", myacct/(test_set*2-2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnosing the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's plot error vs polynomial features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for training and cv set costs \n",
    "\n",
    "train_costo = []\n",
    "cv_costo = []\n",
    "\n",
    "order_array = []\n",
    "\n",
    "# Set regularisation parameter\n",
    "lam = 0.0\n",
    "\n",
    "# Populate the arrays  \n",
    "for i in range(1,20):\n",
    "    myorder = i \n",
    "    # Training  and CV vectors \n",
    "    mytrainmat=map_features(total_low,total_high,myorder)\n",
    "    mycvmat=map_features(total_low_cv,total_high_cv,myorder)\n",
    "    \n",
    "    # initial weight matrix \n",
    "    winit = np.zeros(len(mytrainmat))\n",
    "\n",
    "    xopt = optimize.fmin(mycost, winit , args=(mytrainmat, training_y , m*2, lam), xtol=1e-10, disp=True)\n",
    "    \n",
    "    traincost = mycost(xopt, mytrainmat , training_y , m*2, lam)  \n",
    "    cvcost = mycost(xopt, mycvmat , cv_y , (cv_set-1)*2, lam)  \n",
    "    \n",
    "    train_costo.append(traincost)\n",
    "    cv_costo.append(cvcost)\n",
    "    order_array.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "#Pure spectra \n",
    "plt.plot(order_array ,train_costo, color='b', linestyle='dashed', marker='x', label='Train')\n",
    "plt.plot(order_array ,cv_costo, color='g',  linestyle='dashed', marker='o',  label= 'Cross-validation')\n",
    "\n",
    "plt.axvline(x=12, ymin=0., ymax=10,color='k',linestyle='dotted')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.ylabel(' J$(\\\\theta)$ ',**font1)\n",
    "plt.xlabel('Order of Polynomial', **font1)\n",
    "\n",
    "plt.legend(loc='best',prop={'size': 14})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot costs vs lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for training and cv set costs \n",
    "\n",
    "train_costl = []\n",
    "cv_costl = []\n",
    "\n",
    "lam_array = []\n",
    "\n",
    "# Set Order\n",
    "myorder = 5\n",
    "\n",
    "# Populate the arrays  \n",
    "for i in range(1,30):\n",
    "    lam = 0.0 + 0.1*i\n",
    "    # Training  and CV vectors \n",
    "    mytrainmat=map_features(total_low,total_high,myorder)\n",
    "    mycvmat=map_features(total_low_cv,total_high_cv,myorder)\n",
    "    \n",
    "    # initial weight matrix \n",
    "    winit = np.zeros(len(mytrainmat))\n",
    "\n",
    "    xopt = optimize.fmin(mycost, winit , args=(mytrainmat, training_y , m*2, lam), xtol=1e-10, disp=True)\n",
    "    \n",
    "    traincost = mycost(xopt, mytrainmat , training_y , m*2, lam)  \n",
    "    cvcost = mycost(xopt, mycvmat , cv_y , (cv_set-1)*2, lam)  \n",
    "    \n",
    "    train_costl.append(traincost)\n",
    "    cv_costl.append(cvcost)\n",
    "    lam_array.append(lam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "#Pure spectra \n",
    "plt.plot(lam_array ,train_costl, color='b', linestyle='dashed', marker='x', label='Train')\n",
    "plt.plot(lam_array ,cv_costl, color='g',  linestyle='dashed', marker='o',  label= 'Cross-validation')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.ylabel(' J$(\\\\theta)$ ',**font1)\n",
    "plt.xlabel('$\\lambda$', **font1)\n",
    "\n",
    "plt.legend(loc='best',prop={'size': 16})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment: Classify the skewed data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some examples to investigate \n",
    "exampleno = 2\n",
    "\n",
    "#Columns: k , P(k) w BAO feature , P(k) w/o BAO feature\n",
    "fr_exp = np.loadtxt('data/class/skewed/fr/'+str(exampleno)+'.txt')\n",
    "gr_exp = np.loadtxt('data/class/skewed/gr/'+str(exampleno)+'.txt')\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "ax = plt.subplot(1,2,1)\n",
    "#Pure spectra \n",
    "plt.plot(fr_exp[:,0],fr_exp[:,1], color='r')\n",
    "plt.plot(gr_exp[:,0],gr_exp[:,1], color='b')\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.ylabel(' $P(k)$ ',**font1)\n",
    "plt.xlabel('$k [h/{\\\\rm Mpc}]$', **font1)\n",
    "\n",
    "ax = plt.subplot(1,2,2)\n",
    "#BAO signal \n",
    "plt.plot(fr_exp[:,0],fr_exp[:,1]/fr_exp[:,2], color='r', label='f(R)')\n",
    "plt.plot(gr_exp[:,0],gr_exp[:,1]/gr_exp[:,2], color='b', label='GR')\n",
    "\n",
    "\n",
    "\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "plt.ylabel(' BAO signal ',**font1)\n",
    "plt.xlabel('$k [h/{\\\\rm Mpc}]$', **font1)\n",
    "plt.legend(loc='best',prop={'size': 16})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Maximum and minimum index to find maximum and minimum of spectrum - our selected feature\n",
    "maxi = 20\n",
    "mini = 10\n",
    "\n",
    "kvalues = fr_exp[mini:maxi,0]\n",
    "print(kvalues)\n",
    "\n",
    "#Plot the range over which we are defining our features \n",
    "plt.plot(fr_exp[:,0],fr_exp[:,1]/fr_exp[:,2], color='r', label='f(R)')\n",
    "plt.plot(gr_exp[:,0],gr_exp[:,1]/gr_exp[:,2], color='b', label='GR')\n",
    "\n",
    "plt.axvline(x=kvalues[0], ymin=0., ymax=10,color='k',linestyle='dotted')\n",
    "plt.axvline(x=kvalues[-1], ymin=0., ymax=10,color='k',linestyle='dotted')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Skewed data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of examples per class \n",
    "import os\n",
    "path, dirs, files = next(os.walk(\"./data/class/skewed/fr\"))\n",
    "file_count = len(files)\n",
    "\n",
    "pathgr, dirsgr, filesgr = next(os.walk(\"./data/class/skewed/gr\"))\n",
    "file_countgr = len(filesgr)\n",
    "\n",
    "# number of files to be included in test set \n",
    "test_set = 101\n",
    "test_setgr = 51\n",
    "\n",
    "# number of files to be included in cross-validation set \n",
    "cv_set = 201\n",
    "cv_setgr = 151\n",
    "\n",
    "\n",
    "# F(R) training set size \n",
    "m1 = file_count-test_set-cv_set\n",
    "\n",
    "# GR training set size \n",
    "m2 = file_countgr-test_setgr-cv_setgr\n",
    "\n",
    "\n",
    "# Define an array to store the various values of P(k) at a low k and another at a high k\n",
    "high_fr = []\n",
    "high_gr = []\n",
    "low_fr = []\n",
    "low_gr = []\n",
    "\n",
    "# Load the P(k) values per k \n",
    "for i in range(m1):\n",
    "    datfr = np.loadtxt('data/class/fr_mlg/'+str(i+1)+'.txt')\n",
    "    datgr = np.loadtxt('data/class/gr_mlg/'+str(i+1)+'.txt')\n",
    "    \n",
    "    #perform rough feature normalisation \n",
    "    norm = np.mean(datgr[mini:maxi,1])\n",
    "    \n",
    "    # add the features to the feature vector \n",
    "    high_fr.append(np.max(datfr[mini:maxi,1]/norm))\n",
    "    low_fr.append(np.min(datfr[mini:maxi,1]/norm))  \n",
    "\n",
    "for i in range(m2):\n",
    "\n",
    "    datgr = np.loadtxt('data/class/gr_mlg/'+str(i+1)+'.txt')\n",
    "   \n",
    "    norm = np.mean(datgr[mini:maxi,1])\n",
    "\n",
    "    high_gr.append(np.max(datgr[mini:maxi,1]/norm))\n",
    "    low_gr.append(np.min(datgr[mini:maxi,1]/norm))\n",
    "    \n",
    "    \n",
    "total_low = np.concatenate((low_gr,low_fr), axis=None)\n",
    "total_high = np.concatenate((high_gr,high_fr), axis=None)\n",
    "\n",
    "\n",
    "plt.plot(low_fr,high_fr, linestyle='none',marker='x', label='f(R)')\n",
    "plt.plot(low_gr,high_gr, linestyle='none',marker='o', label='GR')\n",
    "plt.ylabel('Maximum $P(k)$ ',**font1)\n",
    "plt.xlabel('Minimum $P(k)$', **font1)\n",
    "plt.legend(loc='best',prop={'size': 16})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
